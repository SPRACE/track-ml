{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# include path to our library\n",
    "cwd = os.getcwd()\n",
    "dir_path = os.path.dirname(os.path.realpath(cwd))\n",
    "sys.path.append(dir_path)\n",
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path\n",
    "%cd $dir_path\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import set_random_seed\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "from core.data.data_loader import *\n",
    "from core.models.lstm import ModelLSTM, ModelLSTMParalel, ModelLSTMCuDnnParalel\n",
    "from core.models.cnn import ModelCNN\n",
    "from core.models.mlp import ModelMLP\n",
    "\n",
    "from core.utils.metrics import *\n",
    "from core.utils.utils import *\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loading Configurartion ##\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load some configurations configurations of model and others\n",
    "configs = json.load(open('config_lstm.json', 'r'))\n",
    "output_path = configs['paths']['save_dir']\n",
    "output_logs = configs['paths']['log_dir']\n",
    "\n",
    "if os.path.isdir(output_path) == False:\n",
    "    os.mkdir(output_path)\n",
    "\n",
    "if os.path.isdir(output_logs) == False:\n",
    "    os.mkdir(output_logs)        \n",
    "\n",
    "time_steps =  configs['model']['layers'][0]['input_timesteps']  # the number of points or hits\n",
    "num_features = configs['model']['layers'][0]['input_features']  # the number of features of each hits\n",
    "split_data = configs['data']['train_split']  # the number of features of each hits\n",
    "cylindrical = configs['data']['cylindrical']  # set to polar or cartesian coordenates\n",
    "#cylindrical = False\n",
    "normalise = configs['data']['normalise'] \n",
    "num_hits = configs['data']['num_hits']\n",
    "\n",
    "print('OK reading of json file')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading path of data\n",
    "path = configs['data']['filename']\n",
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = Dataset(path, split_data, cylindrical, num_hits, KindNormalization.Zscore)\n",
    "# divimos \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.data_train.iloc[0:12, 0:18]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = data.get_training_data(n_hit_in=time_steps, n_hit_out=1,\n",
    "                                 n_features=num_features, normalise=True)\n",
    "#dataset = dataset.iloc[0:2640,0:]/   b\n",
    "#dataset = dataset.iloc[0:31600,0:]\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "X_train.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save numpy array as npy file\n",
    "from numpy import asarray\n",
    "from numpy import save\n",
    "from numpy import load\n",
    "\n",
    "import os\n",
    "\n",
    "def save_scale_param(path):\n",
    "    #return data.x_scaler.mean_,  data.x_scaler.var_, data.y_scaler.mean_, data.y_scaler.var_\n",
    "    save(os.path.join(path, 'x_scaler_scale.npy'), asarray(data.x_scaler.scale_))\n",
    "    save(os.path.join(path, 'x_scaler_mean.npy'), asarray(data.x_scaler.mean_))\n",
    "    save(os.path.join(path, 'x_scaler_var.npy'), asarray(data.x_scaler.var_))\n",
    "    \n",
    "    save(os.path.join(path, 'y_scaler_scale.npy'), asarray(data.y_scaler.scale_))    \n",
    "    save(os.path.join(path, 'y_scaler_mean.npy'), asarray(data.y_scaler.mean_))\n",
    "    save(os.path.join(path, 'y_scaler_var.npy'), asarray(data.y_scaler.var_))\n",
    "\n",
    "def load_scale_param(path):\n",
    "    x_scaler = StandardScaler()\n",
    "    y_scaler = StandardScaler()\n",
    "\n",
    "    x_scale = load(path + '/x_scaler_scale.npy')    \n",
    "    x_mean = load(path + '/x_scaler_mean.npy')\n",
    "    x_var = load(path + '/x_scaler_var.npy')\n",
    "    \n",
    "    y_scale = load(path + '/y_scaler_scale.npy')\n",
    "    y_mean = load(path + '/y_scaler_mean.npy')\n",
    "    y_var = load(path + '/y_scaler_var.npy')\n",
    "    \n",
    "    x_scaler.scale_ = x_scale\n",
    "    x_scaler.mean_ = x_mean\n",
    "    x_scaler.var_ = x_var\n",
    "    \n",
    "    y_scaler.scale_ = y_scale\n",
    "    y_scaler.mean_ = y_mean\n",
    "    y_scaler.var_ = y_var\n",
    "    \n",
    "    return x_scaler, y_scaler\n",
    "    \n",
    "\n",
    "#save_scale_param(output_path)\n",
    "print('--- parameters saved ---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.x_scaler.scale_)\n",
    "print('----X----')\n",
    "print(data.x_scaler.mean_)\n",
    "print(data.x_scaler.var_)\n",
    "print('----Y----')\n",
    "print(data.y_scaler.mean_)\n",
    "print(data.y_scaler.var_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there is empty data?\n",
    "#print(X_train.isnull().values.any())\n",
    "#print(X_test.isnull().values.any())\n",
    "#print(split)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train = data.reshape3d(X_train, time_steps, num_features)\n",
    "\n",
    "#print('[Data] shape X_train ', X_train.shape)\n",
    "#print('[Data] shape y_train ', y_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_models(config):\n",
    "    \n",
    "    type_model = config['model']['name']\n",
    "    model = None\n",
    "\n",
    "    if type_model == 'lstm': #simple LSTM\n",
    "        model = ModelLSTM(config)\n",
    "    elif type_model == 'lstm-paralel':\n",
    "        model = ModelLSTMParalel(config)\n",
    "    elif type_model == 'cnn':\n",
    "        model = ModelCNN(config)\n",
    "    elif type_model == 'mlp':\n",
    "        model = ModelMLP(config)        \n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# geramos o modelo\n",
    "#configs = json.load(open('config_test.json', 'r'))\n",
    "model = manage_models(configs)\n",
    "\n",
    "if model is None:\n",
    "    print('Please instance model')\n",
    "\n",
    "loadModel = configs['training']['load_model']\n",
    "#loadModel = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Model ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.utils.metrics import *\n",
    "\n",
    "# in-memory training\n",
    "if loadModel == False:\n",
    "    model.build_model()\n",
    "    # in-memory training\n",
    "    history = model.train(\n",
    "        x=X_train,\n",
    "        y=y_train,\n",
    "        epochs = configs['training']['epochs'],\n",
    "        batch_size = configs['training']['batch_size']\n",
    "    )\n",
    "\n",
    "    evaluate_training(history, output_path)\n",
    "\n",
    "elif loadModel == True:       \n",
    "    if not model.load_model():\n",
    "        print ('[Error] please change the config file : load_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate The model (just training)##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate the model\n",
    "from core.utils.metrics import *\n",
    "#evaluate_training(history,output_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Prediction and Nearest ##\n",
    "We concatenate features to predict one hit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "import numpy as np\n",
    "from core.utils.metrics import *\n",
    "from core.utils.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not normalized \n",
    "x_scaler, y_scaler = load_scale_param(output_path)\n",
    "\n",
    "X_test, y_test = data.get_testing_data(n_hit_in=time_steps, n_hit_out=1,\n",
    "                                       n_features=num_features, normalise=False,\n",
    "                                       xscaler=x_scaler, yscaler=y_scaler)\n",
    "\n",
    "#X_test, y_test = data.get_testing_data(n_hit_in=time_steps, n_hit_out=1,\n",
    "#                                       n_features=num_features, normalise=False,\n",
    "#                                       xscaler=None, yscaler=None)\n",
    "\n",
    "\n",
    "#X_test = X_test.iloc[0:1000,]\n",
    "#y_test = y_test[0:1000]\n",
    "\n",
    "#orig\n",
    "#data.data_test.head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(y_test.shape)\n",
    "X_test.head(6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test_inv = pd.DataFrame(data.inverse_transform_x(X_test))\n",
    "#X_test_inv.head(12)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# se tivese scaled\n",
    "mat = convert_matrix_to_vec(y_test, 3)\n",
    "scal = data.y_scaler.transform(mat)\n",
    "inv = data.inverse_transform_y(scal)\n",
    "inv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('----X----')\n",
    "print(data.x_scaler.mean_)\n",
    "print(data.x_scaler.var_)\n",
    "print('----Y----')\n",
    "print(data.y_scaler.mean_)\n",
    "print(data.y_scaler.var_)\n",
    "\n",
    "'''\n",
    "----X----\n",
    "[1.64998197e+02 2.46674255e+00 4.51951273e-01 2.41932911e+02\n",
    " 3.31336219e+00 6.24867173e-01 3.38953707e+02 4.11956379e+00\n",
    " 8.44515198e-01 4.55410774e+02 4.76795818e+00 1.08611730e+00]\n",
    "[12336.17859739  1767.83604707   845.07801004 21304.99449295\n",
    "  3074.0853378   1667.91653131 34956.50034433  4889.29844894\n",
    "  3098.07273178 50047.92179565  7236.85455707  5303.03409087]\n",
    "----Y----\n",
    "[595.87921191   5.05573262   1.38532283]\n",
    "[67778.4996686  10586.88738176  8709.42857666]\n",
    "\n",
    "X =data.scale_data(data.data_test.values, \n",
    "                mean=[595.87921191, 5.05573262, 1.38532283],\n",
    "                stds=[67778.4996686, 10586.88738176, 8709.42857666])\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Prediction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X_test_[0]\n",
    "print(x)\n",
    "#x = convert_matrix_to_vec(x, 3)\n",
    "x = pd.DataFrame(x)\n",
    "x = data.inverse_transform_x(x.values.flatten())\n",
    "print(x)\n",
    "x = data.x_scaler.transform(np.reshape(x,(1,12)))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = data.reshape3d(X_test_inv, time_steps, num_features)\n",
    "X_test_[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normal Prediction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_ = data.reshape3d(X_test, time_steps, num_features)\n",
    "y_test_ = data.reshape3d(y_test, 6, num_features)\n",
    "\n",
    "print(X_test_.shape)\n",
    "print(y_test_.shape)\n",
    "print(X_test_[0])\n",
    "print(y_test_[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_full_sequences(x_test, data, num_hits=6, normalise=False):\n",
    "    '''\n",
    "        x_test: input data\n",
    "        normalise: say input data must be scaled \n",
    "    '''\n",
    "    timer = Timer()\n",
    "    timer.start() \n",
    "    print('[Model] Predicting Sequences Started')\n",
    "\n",
    "    total = len(x_test)\n",
    "\n",
    "    correct = 0\n",
    "    incorrect = 0\n",
    "    pred_sequences = []\n",
    "\n",
    "    for j in range(total):       \n",
    "        curr_frame = x_test[j]\n",
    "        predicted = []\n",
    "        for i in range(num_hits):\n",
    "            if normalise:\n",
    "                curr_frame = data.x_scaler.transform(np.reshape(curr_frame,(1,12)))\n",
    "                curr_frame_orig = data.inverse_transform_x(pd.DataFrame(curr_frame).values.flatten())\n",
    "                curr_frame_orig = np.reshape(curr_frame_orig, (4,3))\n",
    "                curr_frame = np.reshape(curr_frame, (4,3))\n",
    "            else:\n",
    "                curr_frame = curr_frame\n",
    "                curr_frame_orig = curr_frame\n",
    "                            \n",
    "            pred = model.model.predict(curr_frame[np.newaxis,:,:])                         \n",
    "            \n",
    "            pred = np.reshape(pred, (1, 3))\n",
    "            if normalise:\n",
    "                pred = data.inverse_transform_y(pred)\n",
    "            else:\n",
    "                pred = pred\n",
    "                \n",
    "            pred = np.reshape(pred, (1, 3))\n",
    "            \n",
    "            predicted.append(pred)\n",
    "            curr_frame = curr_frame_orig[1:]\n",
    "            # inserta um  valor np.insert(array, index, value, axes)\n",
    "            curr_frame = np.insert(curr_frame, [3], predicted[-1], axis=0)\n",
    "            #print(curr_frame, predicted[-1])\n",
    "\n",
    "        pred_sequences.append(predicted)\n",
    "\n",
    "    print('[Model] Prediction Finished.')\n",
    "    timer.stop()\n",
    "\n",
    "    return pred_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalise = True say the input must be normalised\n",
    "pred_full = predict_full_sequences(X_test_, data, num_hits=6, normalise=True)\n",
    "print('finished prediction ...')\n",
    "\n",
    "predicted = convert_vector_to_matrix(pred_full, 3, 6)\n",
    "predicted = to_frame(predicted)\n",
    "predicted.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# original data y_true\n",
    "y_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Nearest ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bag_of_hits = None\n",
    "from enum import Enum\n",
    "\n",
    "def predict_full_sequences_nearest1(x_test, y_test, num_hits=6, num_features=3, normalise=False):\n",
    "    \n",
    "    timer = Timer()\n",
    "    timer.start()       \n",
    "\n",
    "    print('[Model] Predicting Sequences with Nearest Started')\n",
    "\n",
    "    total = len(x_test)\n",
    "\n",
    "    pred_sequences = []\n",
    "\n",
    "    decimals = 2\n",
    "    y_true = None\n",
    "    \n",
    "    #if normalise:\n",
    "    # covert to original values\n",
    "    y_true = data.inverse_transform_test_y(y_test)\n",
    "        \n",
    "    #y_true = y_true.round(decimals)\n",
    "    \n",
    "    #x_test must be in scaled form\n",
    "\n",
    "    global bag_of_hits \n",
    "    bag_of_hits = np.array(convert_matrix_to_vec(y_true, num_features))\n",
    "\n",
    "    y_true = np.array(y_true)\n",
    "    count_correct = np.zeros(num_hits)\n",
    "\n",
    "    for j in range(total):\n",
    "        curr_frame = x_test[j]\n",
    "        curr_track = y_true[j]\n",
    "        \n",
    "        predicted = []\n",
    "        begin = 0\n",
    "        #print('track :', curr_track)\n",
    "        for i in range(num_hits):\n",
    "            end = begin+num_features\n",
    "            curr_hit_orig = curr_track[begin:end]\n",
    "            begin = end\n",
    "            \n",
    "            #print('curr_frame:\\n', curr_frame)\n",
    "            # input must be scaled curr_frame\n",
    "            y_pred = model.model.predict(curr_frame[np.newaxis,:,:])\n",
    "\n",
    "            #print('pred:', y_pred)\n",
    "            y_pred = np.reshape(y_pred, (1, 3))\n",
    "            y_pred_orig = data.inverse_transform_y(y_pred)\n",
    "            #y_pred_orig = np.round(y_pred_orig, decimals)\n",
    "            #print('pred:', y_pred)\n",
    "            \n",
    "            print('BoH: %s, %s'%(len(bag_of_hits), bag_of_hits))\n",
    "\n",
    "            near_pred_orig, idx = model.nearest_hit(y_pred_orig, bag_of_hits, silent=True)\n",
    "            \n",
    "            #print('inv pred:', y_pred_orig)\n",
    "            #print('nearest:', near_pred_orig)\n",
    "            #print('current:', curr_hit_orig)\n",
    "\n",
    "            d1 = calculate_distances_vec(curr_hit_orig, y_pred_orig[0])\n",
    "            d2 = calculate_distances_vec(curr_hit_orig, near_pred_orig)\n",
    "            \n",
    "            #print('dist pred: %s  dist near : %s ' % (d1, d2))\n",
    "            \n",
    "            #bag_of_hits = np.delete(bag_of_hits, idx, 0)\n",
    "            \n",
    "            #if np.logical_and(curr_hit, near_pred).all():\n",
    "            #if np.array_equal(curr_hit, near_pred):\n",
    "                          \n",
    "            if d1 <= d2:\n",
    "                # original coordinates\n",
    "                if np.isclose(curr_hit_orig, y_pred_orig, atol=0.09).all():    \n",
    "                    count_correct[i]=+1\n",
    "                    \n",
    "                predicted.append(y_pred)\n",
    "            else:\n",
    "                # original coordinates\n",
    "                if np.isclose(curr_hit_orig, near_pred_orig, atol=0.09).all():    \n",
    "                    count_correct[i]=+1\n",
    "                               \n",
    "                near_pred = data.y_scaler.transform(np.reshape(near_pred_orig, (1, 3)))\n",
    "                #print('current inv:', near_pred)\n",
    "                predicted.append(near_pred)\n",
    "                      \n",
    "            curr_frame = curr_frame[1:]\n",
    "            # inserta um  valor np.insert(array, index, value, axes)\n",
    "            curr_frame = np.insert(curr_frame, [3], predicted[-1], axis=0)\n",
    "        \n",
    "        pred_sequences.append(predicted)\n",
    "        if j == 2: break\n",
    "    print('[Model] Prediction Finished.')\n",
    "    timer.stop()\n",
    "\n",
    "    return pred_sequences, count_correct\n",
    "\n",
    "def nearest_hit(hit, hits,\n",
    "                    silent = True,\n",
    "                    metric = 'euclidean'):\n",
    "\n",
    "    if silent is False:\n",
    "        timer = Timer()\n",
    "        timer.start()\n",
    "\n",
    "    dist = distance.cdist(hits, hit, metric)\n",
    "\n",
    "    # get the index of minimum distance\n",
    "    target_hit_index = np.argmin(dist)\n",
    "\n",
    "    if silent is False:    \n",
    "        print(\"--- N_hits: %s\" % len(hits))\n",
    "        print(\"--- Hit index: %s\" % target_hit_index)\n",
    "        print(\"--- \" + str(metric) + \" distance: \" + str(dist[target_hit_index]))\n",
    "        print(\"--- time: %s seconds\" % timer.stop())\n",
    "\n",
    "    # get hits coordinates \n",
    "    real_hit = hits[target_hit_index, :]\n",
    "    real_hit = np.array(real_hit)\n",
    "\n",
    "    # removing the hit from bag\n",
    "    #hits = np.delete(hits, target_hit_index, 0)\n",
    "\n",
    "    if dist_hit is False:\n",
    "        return real_hit, target_hit_index\n",
    "    else:\n",
    "        return real_hit, target_hit_index, np.min(dist)\n",
    "\n",
    "def mat_rhoetaphi_to_xyz(mat, n_hits = 5):\n",
    "    pivot_tmp = 0\n",
    "    for i in range(n_hits):\n",
    "        pivot_tmp = i * 3\n",
    "        rho = mat[pivot_tmp + 0]\n",
    "        eta = mat[pivot_tmp + 1]\n",
    "        phi = mat[pivot_tmp + 2]\n",
    "        if (rho != 0 and eta != 0 and phi != 0):\n",
    "            x, y, z = convert_rhoetaphi_to_xyz(rho, eta, phi)\n",
    "            mat[pivot_tmp + 0] = x\n",
    "            mat[pivot_tmp + 1] = y\n",
    "            mat[pivot_tmp + 2] = z\n",
    "    return mat\n",
    "\n",
    "class BagOfHits(Enum):\n",
    "    All=1,\n",
    "    Track=2,\n",
    "    Layer=3\n",
    "    \n",
    "def predict_full_sequences_nearest2(x_test, y_test, bag_of_hits, y_test_aux=None, num_hits=6, num_features=3,\n",
    "                                    normalise=False, cylindrical=False, verbose=False, tol=0.01):\n",
    "    \n",
    "    '''\n",
    "        This function shift the window by 1 new prediction each time, re-run predictions on new window\n",
    "        parameters:\n",
    "        x_test : X test data normaly not scaled (4 hits)\n",
    "        y_test : y test data, normaly not scaled (6 hits)\n",
    "        num_hits : how many hits by y_test\n",
    "        normalise : it param says the input data must be scaled or not\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    timer = Timer()\n",
    "    timer.start()\n",
    "\n",
    "    print('[Model] Predicting Sequences with Nearest Started')\n",
    "\n",
    "    total = len(x_test)\n",
    "\n",
    "    pred_sequences = []\n",
    "    pred_sequences_orig = []\n",
    "    \n",
    "    decimals = 2\n",
    "   \n",
    "    # covert to original values\n",
    "    #y_true = data.inverse_transform_test_y(y_test)\n",
    "    #y_true = y_true.round(decimals)\n",
    "    \n",
    "    #change the dataset by cartesian coordinates\n",
    "    if cylindrical:\n",
    "        y_test = y_test_aux\n",
    "    else:\n",
    "        y_test = y_test\n",
    "        \n",
    "    # bag_of_hits  \n",
    "    bag_of_hits_all = np.array(convert_matrix_to_vec(y_test, num_features))\n",
    "\n",
    "    count_correct = np.zeros(num_hits)\n",
    "    begin_idx, end_idx = 0, 0\n",
    "    num_boh = 6\n",
    "    \n",
    "    for j in range(total):\n",
    "        curr_frame = x_test[j]\n",
    "        # bag_of_hit by track\n",
    "        curr_track = np.array(y_test.iloc[j,0:]).reshape(num_hits, num_features)\n",
    "        \n",
    "        if verbose:\n",
    "            print('curr_track %s , %s:' % (j , curr_track))\n",
    "\n",
    "        predicted = []\n",
    "        predicted_orig = []\n",
    "        begin = 0\n",
    "        for i in range(num_hits):\n",
    "            # bag_of_hit by layer\n",
    "            end = begin+num_features\n",
    "            curr_layer = np.array(y_test.iloc[0:,begin:end]).reshape(total, num_features)\n",
    "            curr_hit = curr_track[i]\n",
    "            begin = end\n",
    "            \n",
    "            if verbose:\n",
    "                # primeira esta em originais\n",
    "                print('input:\\n', curr_frame)\n",
    "            \n",
    "            if normalise:\n",
    "                curr_frame = data.x_scaler.transform(np.reshape(curr_frame,(1,12)))\n",
    "                curr_frame_orig = data.inverse_transform_x(pd.DataFrame(curr_frame).values.flatten())\n",
    "                curr_frame_orig = np.reshape(curr_frame_orig, (4,3))\n",
    "                curr_frame = np.reshape(curr_frame, (4,3))\n",
    "            else:\n",
    "                curr_frame = curr_frame\n",
    "                curr_frame_orig = curr_frame\n",
    "                \n",
    "            if verbose:\n",
    "                print('input:\\n', curr_frame)\n",
    "                #print('input orig:\\n', curr_frame_orig)\n",
    "            \n",
    "            #print('input inv :\\n', curr_frame_inv.reshape(4,3))\n",
    "            #print('newaxis ', curr_frame[np.newaxis,:,:])\n",
    "            # input must be scaled curr_frame\n",
    "            y_pred = model.model.predict(curr_frame[np.newaxis,:,:])\n",
    "\n",
    "            \n",
    "            y_pred = np.reshape(y_pred, (1, 3))\n",
    "            if normalise:\n",
    "                y_pred_orig = data.inverse_transform_y(y_pred)\n",
    "            else:\n",
    "                y_pred_orig = y_pred\n",
    "\n",
    "            y_pred_orig = np.reshape(y_pred_orig, (1, 3))\n",
    "\n",
    "            if bag_of_hits == BagOfHits.All:\n",
    "                hits = bag_of_hits_all\n",
    "            elif bag_of_hits == BagOfHits.Track:\n",
    "                hits = curr_track\n",
    "            elif bag_of_hits == BagOfHits.Layer:                    \n",
    "                hits = curr_layer\n",
    "            \n",
    "            if cylindrical:\n",
    "                rho, eta, phi = y_pred_orig[0][0], y_pred_orig[0][1], y_pred_orig[0][2]\n",
    "                #print(rho,eta,phi)\n",
    "                x, y, z = convert_rhoetaphi_to_xyz(rho, eta, phi)\n",
    "                #print(x,y,z)\n",
    "                y_pred_orig[0][0] = x\n",
    "                y_pred_orig[0][1] = y\n",
    "                y_pred_orig[0][2] = z\n",
    "                \n",
    "                #print(y_pred_orig)\n",
    "                    \n",
    "            dist = distance.cdist(hits, y_pred_orig, 'euclidean')\n",
    "            idx = np.argmin(dist)\n",
    "            near_pred = hits[idx]\n",
    "\n",
    "            # if curr_hit is in cartesian coord, near_pred must be in cartesian coord too\n",
    "            if np.isclose(curr_hit, near_pred, atol=tol).all():\n",
    "                count_correct[i]+=1\n",
    "                #print('count_correct[%s] = %s ' % (i, count_correct[i]))\n",
    "                \n",
    "            if verbose:\n",
    "                print('pred:', y_pred)\n",
    "                print('inv pred:', y_pred_orig)\n",
    "                print('current:', curr_hit)\n",
    "                print('nearest:', near_pred)\n",
    "            \n",
    "            if cylindrical:\n",
    "                x, y, z = near_pred[0], near_pred[1], near_pred[2]\n",
    "                #print(x,y,z)\n",
    "                rho, eta, phi = convert_xyz_to_rhoetaphi(x, y, z)\n",
    "                #print(rho,eta,phi)\n",
    "                near_pred[0] = rho\n",
    "                near_pred[1] = eta\n",
    "                near_pred[2] = phi\n",
    "                \n",
    "            #curr_track = np.delete(curr_track, idx, 0)\n",
    "            #near_pred_orig, idx = model.nearest_hit(y_pred_orig, bag_of_hits, silent=True)\n",
    "            \n",
    "           \n",
    "            \n",
    "            '''            \n",
    "            d1 = calculate_distances_vec(curr_hit, y_pred_orig[0])\n",
    "            d2 = calculate_distances_vec(curr_hit, near_pred)\n",
    "            #print('d1 %s, d2 %s' % (d1, d2))\n",
    "            \n",
    "            if d1 <= d2:\n",
    "                # original coordinates\n",
    "                if np.isclose(curr_hit, y_pred_orig, atol=0.09).all():    \n",
    "                    count_correct[i]=+1\n",
    "                #print('d1')    \n",
    "                predicted.append(y_pred)\n",
    "                predicted_orig.append(y_pred_orig)\n",
    "            else:\n",
    "                # original coordinates\n",
    "                if np.isclose(curr_hit, near_pred_orig, atol=0.09).all():    \n",
    "                    count_correct[i]=+1\n",
    "                #print('d2')                                         \n",
    "                near_pred_scaled = data.y_scaler.transform(np.reshape(near_pred, (1, 3)))\n",
    "                print('near scaled:', near_pred_scaled)\n",
    "                predicted.append(near_pred_scaled)\n",
    "                predicted_orig.append(near_pred)\n",
    "            '''\n",
    "            \n",
    "            '''\n",
    "            near_pred = np.reshape(near_pred, (1, 3))\n",
    "            if normalise:\n",
    "                near_pred = data.y_scaler.transform(near_pred)\n",
    "            else:\n",
    "                near_pred = near_pred\n",
    "            \n",
    "            if verbose:\n",
    "                print('near scaled:', near_pred)\n",
    "            '''   \n",
    "            \n",
    " \n",
    "                    \n",
    "            #near adiciona em valores originaeis\n",
    "            predicted.append(near_pred)\n",
    "            \n",
    "            #predicted_orig.append(near_pred_orig)\n",
    "            #print('-----\\n')\n",
    "            \n",
    "            curr_frame = curr_frame_orig[1:]\n",
    "            curr_frame = np.insert(curr_frame, [3], predicted[-1], axis=0)\n",
    "            \n",
    "            #curr_frame_new = data.x_scaler.transform(np.reshape(curr_frame_inv, (1, 12)))\n",
    "            #curr_frame_new_inv = data.inverse_transform_x(pd.DataFrame(curr_frame).values.flatten())\n",
    "            \n",
    "            #print('new frame ', curr_frame_new.reshape(4,3))\n",
    "            #print('new frame inv ', curr_frame_new_inv.reshape(4,3))\n",
    "            \n",
    "        pred_sequences.append(predicted)\n",
    "        #pred_sequences_orig.append(predicted_orig)\n",
    "        \n",
    "        if verbose:\n",
    "            if j == 3: break\n",
    "                \n",
    "    print('[Model] Prediction Finished.')\n",
    "    timer.stop()\n",
    "\n",
    "    return pred_sequences, count_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test = X_test.iloc[0:1000,]\n",
    "#y_test = y_test[0:1000]\n",
    "X_test_ = data.reshape3d(X_test, time_steps, num_features)\n",
    "y_test_ = data.reshape3d(y_test, 6, num_features)\n",
    "\n",
    "print(X_test_.shape)\n",
    "print(y_test_.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pred_full_res, correct = predict_full_sequences_nearest1(X_test_, y_test, 6)\n",
    "#print('finished prediction nearest...')\n",
    "\n",
    "#predicted_nearest = convert_vector_to_matrix(pred_full_res, 3, 6)\n",
    "#predicted_nearest = to_frame(predicted_nearest)\n",
    "#predicted_nearest.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aux data\n",
    "data_tmp = Dataset(path, split_data, False, num_hits, KindNormalization.Zscore)\n",
    "X_test_aux, y_test_aux = data_tmp.get_testing_data(n_hit_in=time_steps, n_hit_out=1,\n",
    "                                 n_features=num_features, normalise=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_aux = X_test_aux.iloc[0:1000,]\n",
    "y_test_aux = y_test_aux[0:1000]\n",
    "X_test_aux.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# normalise = True say input data must be scaled or not \n",
    "pred_full_res, correct = predict_full_sequences_nearest2(X_test_, y_test, BagOfHits.Layer, None, 6, \n",
    "                                                         normalise=True, cylindrical=False,\n",
    "                                                         verbose=False, tol=0.01)\n",
    "\n",
    "predicted_nearest = convert_vector_to_matrix(pred_full_res, 3, 6)\n",
    "predicted_nearest = to_frame(predicted_nearest)\n",
    "predicted_nearest.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vetor de contagens\n",
    "total = X_test.shape[0]\n",
    "print(correct)\n",
    "#taxa_acertos = [(correct*100)/total for x in range(0, len(correct))]\n",
    "print((correct*100)/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "#plt.bar(range(1,6), correct)\n",
    "layers = ['layer 5', 'layer 6','layer 7','layer 8','layer 9','layer 10']\n",
    "\n",
    "#plt.bar(np.arange(1, 6, 1), correct)\n",
    "plt.bar(layers, correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_track 0 , [[220.0039978  143.64300537  13.80000019]\n",
    " [309.77398682 192.727005    22.20000076]\n",
    " [433.60699463 252.54699707  35.40000153]\n",
    " [586.02099609 313.86199951  49.79999924]\n",
    " [735.70300293 362.45001221  65.19999695]\n",
    " [935.93103027 410.11999512  86.80000305]]:\n",
    "input:    \n",
    "[[ 25.9116993   18.92830086  -6.89595985]\n",
    " [ 59.46440125  42.51089859  -3.20423007]\n",
    " [ 94.53399658  66.21589661   0.57812101]\n",
    " [142.01199341  96.84390259   5.62239981]]    \n",
    "\n",
    "curr_track 1 :    \n",
    "[[ 257.53500366   17.9375      128.80000305]\n",
    " [ 361.7749939    29.22640038  182.19999695]\n",
    " [ 497.4670105    47.53120041  251.80000305]\n",
    " [ 656.45397949   73.98660278  333.6000061 ]\n",
    " [ 814.44396973  105.75900269  417.3999939 ]\n",
    " [1007.89001465  152.21299744  520.40002441]]:  \n",
    "input:\n",
    " [[ 31.56760025   1.38691998  13.81000042]\n",
    " [ 72.24410248   3.5746901   34.45349884]\n",
    " [115.68099976   6.29080009  56.51910019]\n",
    " [171.48800659  10.3526001   84.90619659]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_test_inv = data.inverse_transform_test_y(y_test)\n",
    "y_test_inv = pd.DataFrame(y_test_inv)\n",
    "\n",
    "\n",
    "boh_layer = np.array(y_test_inv.iloc[0:,0:3]).reshape(len(y_test_inv),3)\n",
    "boh_track = np.array(y_test_inv.iloc[0,0:]).reshape(6,3)\n",
    "boh_all = np.array(convert_matrix_to_vec(y_test_inv, 3))\n",
    "print(boh_track[1])\n",
    "\n",
    "\n",
    "#current = [[ 331.34717,  -142.58179,    -6.709302]] \n",
    "pred = [[ 235.75716,  -109.62279,    -3.109302]]\n",
    "n = model.nearest_hit(pred, boh_all, silent=False)\n",
    "n\n",
    "\n",
    "\n",
    "#inv pred: [[ 235.75716  -109.62279    -3.109302]]\n",
    "#nearest: [ 236.97200012 -107.73600006   -3.        ]\n",
    "#current: [ 236.97200012 -107.73600006   -3.        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data.inverse_transform_test_y(y_test)\n",
    "#y = np.array(y)\n",
    "#y = pd.DataFrame(y)\n",
    "#y\n",
    "\n",
    "#track = pd.DataFrame(y[0])\n",
    "#t = np.reshape(track, 1, 18)\n",
    "#t\n",
    "#np.array(track)\n",
    "\n",
    "boh = np.array(convert_matrix_to_vec(y, num_features))\n",
    "#boh[0:6]\n",
    "boh\n",
    "\n",
    "#boh = convert_matrix_to_vec(track, num_features)\n",
    "#\n",
    "#boh\n",
    "#print(y_test.shape)\n",
    "#print(len(boh))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "'''\n",
    "current_track = np.array([[236.97, -107.74, -3.],\n",
    "                 [332.56, -140.7, -6.6],\n",
    "                 [468.9,  -177.86,  -10.2],\n",
    "                 [630.02, -208.33,  -15.],\n",
    "                 [791.81, -224.49,  -16.2],\n",
    "                 [997.68,-224.02,  -27.]])\n",
    "'''\n",
    "\n",
    "tracks = np.array([\n",
    " [ 332.56201172, -140.69500732,   -6.5999999 ],\n",
    " [ 468.89599609, -177.86099243,  -10.19999981],\n",
    " [ 630.02398682, -208.33099365,  -15.        ],\n",
    " [ 791.80603027, -224.48699951,  -16.20000076],\n",
    " [ 997.67901611, -224.02400208,  -27.        ]])\n",
    "\n",
    "current = [[236.97200012, -107.73600006, -3. ]]\n",
    "pred = [[239.28717, -102.406906, -5.2484846]]\n",
    "pred = [[ 245.10965, -117.84069,   -6.3022523]]\n",
    "n = model.nearest_hit(pred, tracks, silent=False)\n",
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "near = np.array([[332.56, -140.7, -6.6 ]])\n",
    "scaled = data.y_scaler.transform(near)\n",
    "print(scaled)\n",
    "orig = data.inverse_transform_y(scaled)\n",
    "print(orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " [[-1.22701239e+00  4.67683517e-01 -4.26419735e-02]\n",
    " [-1.28115966e+00  5.49873417e-01 -1.11877660e-04]\n",
    " [-1.37037903e+00  6.30455206e-01  3.19417376e-02]\n",
    " [-1.40640257e+00  7.44980250e-01  6.58394857e-02]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convertimos a matriz do test em um vetor \n",
    "#y_test_ = convert_matrix_to_vec(y_test, 3)\n",
    "#y_test_ = np.array(y_test_)\n",
    "\n",
    "pred_full_res, correct = model.predict_full_sequences_nearest(X_test_, y_test, 6)\n",
    "print('finished prediction nearest...')\n",
    "\n",
    "predicted_nearest = convert_vector_to_matrix(pred_full_res, 3, 6)\n",
    "predicted_nearest = to_frame(predicted_nearest)\n",
    "predicted_nearest.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_score1(y_true, y_predicted, report=False):\n",
    "\n",
    "    r2 = r2_score(y_true, y_predicted)\n",
    "    mse = mean_squared_error(y_true, y_predicted)\n",
    "    rmse = sqrt(mean_squared_error(y_true, y_predicted))\n",
    "    mae = mean_absolute_error(y_true, y_predicted)\n",
    "    \n",
    "    report_string = \"\"\n",
    "    report_string += \"---Regression Scores--- \\n\"\n",
    "    report_string += \"\\tR_2 statistics        (R2)  = \" + str(round(r2,3)) + \"\\n\"\n",
    "    report_string += \"\\tMean Square Error     (MSE) = \" + str(round(mse,3)) + \"\\n\"\n",
    "    report_string += \"\\tRoot Mean Square Error(RMSE) = \" + str(round(rmse,3)) + \"\\n\"\n",
    "    report_string += \"\\tMean Absolute Error   (MAE) = \" + str(round(mae,3)) + \"\\n\"\n",
    "\n",
    "    if report:\n",
    "        return r2, mse, rmse, mae, report_string\n",
    "    else:\n",
    "        return r2, mse, rmse, mae\n",
    "\n",
    "def calc_score_layer(y_true, y_pred, n_features):\n",
    "    rows = y_true.shape[0]\n",
    "    cols = y_true.shape[1]\n",
    "    begin, end = 0, 0\n",
    "    layer = 5\n",
    "    for i in range(0, cols, n_features):\n",
    "        end = i + n_features\n",
    "        layer_true = y_true.iloc[0:,begin:end]\n",
    "        layer_pred = y_pred.iloc[0:,begin:end]\n",
    "        begin = end\n",
    "        \n",
    "        r2, mse, rmse, mae, result = calc_score1(data.reshape2d(layer_true, 1),\n",
    "                            data.reshape2d(layer_pred, 1), report=True)   \n",
    "        \n",
    "        print('layer ', layer)\n",
    "        print(result)\n",
    "        layer+=1\n",
    "\n",
    "def calc_score_layer_axes(y_true, y_predicted, features=3):\n",
    "    '''\n",
    "        Return \n",
    "            receive a sequence by track and calculate the score by layer\n",
    "            score  : return the score total with RMSE\n",
    "            scores : return the score RMSE for each features\n",
    "    '''\n",
    "    mses = []\n",
    "    rmses = []\n",
    "    r2s = []\n",
    "    \n",
    "    y_true = np.array(y_true)\n",
    "    y_predicted = np.array(y_predicted)\n",
    "    \n",
    "    rows = y_true.shape[0]\n",
    "    cols = y_true.shape[1]\n",
    "\n",
    "    mse_x, mse_y, mse_z = 0,0,0\n",
    "    r2_x, r2_y, r2_z = 0,0,0\n",
    "    counter = 0\n",
    "    for j in range(0, cols, features):\n",
    "        x, y, z = (j + 0),  (j + 1), (j + 2)\n",
    "        # calculate by column\n",
    "        mse_x+= mean_squared_error(y_true[:,x], y_predicted[:,x])\n",
    "        mse_y+= mean_squared_error(y_true[:,y], y_predicted[:,y])\n",
    "        mse_z+= mean_squared_error(y_true[:,z], y_predicted[:,z])\n",
    "\n",
    "        # adding mse by layer\n",
    "        mses.append([mse_x, mse_y, mse_z])\n",
    "        rmses.append([sqrt(mse_x), sqrt(mse_y), sqrt(mse_z)])\n",
    "        \n",
    "        r2_x+= r2_score(y_true[:,x], y_predicted[:,x])\n",
    "        r2_y+= r2_score(y_true[:,y], y_predicted[:,y])\n",
    "        r2_z+= r2_score(y_true[:,z], y_predicted[:,z])\n",
    "        r2s.append([r2_x, r2_y, r2_z])\n",
    "        \n",
    "        counter+=1\n",
    "        #mse = mean_squared_error(y_true[:,i], y_predicted[:,i])\n",
    "        \n",
    "        #mse = mean_squared_error(y_true[i,j:end_idx], y_predicted[i,j:end_idx])      \n",
    "        #r2 = r2_score(y_true[:,i], y_predicted[:,i])\n",
    "        #r2 = r2_score(y_true[i,j:end_idx], y_predicted[i,j:end_idx])        \n",
    "\n",
    "    #rmses.append(sqrt(mse_x/(rows)))  \n",
    "    #rmses.append(sqrt(mse_y/(rows)))\n",
    "    #rmses.append(sqrt(mse_z/(rows))) \n",
    "    \n",
    "    #print(r2_x, r2_y, r2_z)\n",
    "    #r2s.append(r2_x/(rows))\n",
    "    #r2s.append(r2_y/(rows))\n",
    "    #r2s.append(r2_z/(rows))\n",
    "        \n",
    " \n",
    "\n",
    "    return mses, rmses, r2s\n",
    "\n",
    "def summarize_scores_axes(mses, rmses, r2s):\n",
    "    \n",
    "    counter = 0\n",
    "    for mse, rmse in zip(mses,rmses):\n",
    "        print('layer %s' % counter)\n",
    "        print('\\tMSE:[%.2f, %.2f, %.2f]' % (mse[0], mse[1], mse[2]))\n",
    "        print('\\tRMSE:[%.2f, %.2f, %.2f]' % (rmse[0], rmse[1], rmse[2]))\n",
    "        counter+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for Normal Prediction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# metricas geraeis\n",
    "_,_,_,_,result = calc_score1(data.reshape2d(y_test, 1),\n",
    "                    data.reshape2d(predicted, 1), report=True)\n",
    "print(result)\n",
    "\n",
    "calc_score_layer(y_test, predicted, n_features=3)\n",
    "\n",
    "mses, rmses, r2s = calc_score_layer_axes(y_test, predicted)\n",
    "summarize_scores_axes(mses, rmses, r2s)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics for  Nearest Prediction ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metricas para nearest\n",
    "_,_,_,_,result = calc_score1(data.reshape2d(y_test, 1),\n",
    "                    data.reshape2d(predicted_nearest, 1), report=True)\n",
    "print(result)\n",
    "\n",
    "calc_score_layer(y_test, predicted_nearest, n_features=3)\n",
    "\n",
    "mses, rmses, r2s = calc_score_layer_axes(y_test, predicted_nearest)\n",
    "summarize_scores_axes(mses, rmses, r2s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to Millimeters ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inverse_transform_mat(scaler, data, n_features=3):\n",
    "    '''\n",
    "        Covert a matrix to scaled values by colum hit \n",
    "    '''\n",
    "    rows = data.shape[0]\n",
    "    cols = data.shape[1]\n",
    "    \n",
    "    begin = 0\n",
    "    for i in range(0, cols, n_features):\n",
    "        end = i + n_features\n",
    "        hit_col = data.iloc[0:,begin:end]\n",
    "        begin = end\n",
    "        scaled = scaler.transform(hit_col)\n",
    "        print(scaled)\n",
    "\n",
    "inverse_transform_mat(data.y_scaler, predicted_nearest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalise = False     \n",
    "if normalise:\n",
    "    print('[Data] Inverse Transforming ...')\n",
    "    y_test_orig = data.inverse_transform_test_y(y_test)\n",
    "    y_predicted_orig = data.inverse_transform_test_y(predicted)\n",
    "    y_predicted_orig_nearest = data.inverse_transform_y(predicted_nearest)\n",
    "    \n",
    "    y_test_orig = pd.DataFrame(y_test_orig)    \n",
    "    y_predicted_orig = pd.DataFrame(y_predicted_orig)    \n",
    "else:\n",
    "    y_test_orig = y_test\n",
    "    y_predicted_orig = predicted\n",
    "    \n",
    "    y_predicted_orig_nearest = predicted_nearest\n",
    "    y_predicted_orig_nearest = pd.DataFrame(y_predicted_orig_nearest)\n",
    "\n",
    "print('[Data] shape y_predicted_orig ', y_predicted_orig.shape)    \n",
    "print('[Data] shape y_test_orig ', y_test_orig.shape)\n",
    "\n",
    "result = calc_score(data.reshape2d(y_test_orig, 1),\n",
    "                    data.reshape2d(y_predicted_orig_nearest, 1), report=False)\n",
    "print(result)\n",
    "r2, rmse, rmses = evaluate_forecast_seq(y_test_orig, y_predicted_orig_nearest)\n",
    "summarize_scores(r2, rmse, rmses)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data for Visualization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_true, y_true = data.get_testing_data(n_hit_in=time_steps, n_hit_out=1,\n",
    "                                 n_features=num_features, normalise=False)\n",
    "x_true = x_true.iloc[0:1000,]\n",
    "y_true = y_true[0:1000]\n",
    "\n",
    "#x_true = X_test\n",
    "#y_true = y_test\n",
    "print(x_true.shape)\n",
    "print(y_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join both dataset must be equal to X_test\n",
    "data_true = np.concatenate([x_true , y_true], axis = 1)\n",
    "data_true = pd.DataFrame(data_true)\n",
    "data_true.columns.name = 'real' \n",
    "\n",
    "print('Shape ', data_true.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# must be equal\n",
    "#data_test.values == X_test_new.values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join both dataset should be equal to X_test but not equal\n",
    "data_pred = np.concatenate([x_true , y_predicted_orig], axis = 1 )\n",
    "data_pred = pd.DataFrame(data_pred)\n",
    "data_pred.columns.name = 'predito'\n",
    "\n",
    "print('Shape ', data_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join both dataset should be equal to X_test but not equal\n",
    "data_pred_nearest = np.concatenate([x_true , y_predicted_orig_nearest], axis = 1 )\n",
    "data_pred_nearest = pd.DataFrame(data_pred_nearest)\n",
    "\n",
    "data_pred_nearest.columns.name = 'nearest'\n",
    "\n",
    "print('Shape ', data_pred_nearest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# should not be equal\n",
    "#data_pred.values == X_test_new.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cylindrical:\n",
    "\n",
    "    y_test_orig.to_csv(os.path.join(output_path, 'y_true_%s_cylin_nearest.csv' % configs['model']['name']),\n",
    "                header=False, index=False)\n",
    "\n",
    "    y_predicted_orig.to_csv(os.path.join(output_path, 'y_pred_%s_cylin_nearest.csv' % configs['model']['name']),\n",
    "                header=False, index=False)\n",
    "\n",
    "    x_true.to_csv(os.path.join(output_path, 'x_true_%s_cylin_nearest.csv' % configs['model']['name']),\n",
    "                header=False, index=False)\n",
    "else:\n",
    "    \n",
    "    y_test_orig.to_csv(os.path.join(output_path, 'y_true_%s_xyz_nearest.csv' % configs['model']['name']),\n",
    "                header=False, index=False)\n",
    "    y_predicted_orig.to_csv(os.path.join(output_path, 'y_pred_%s_xyz_nearest.csv' % configs['model']['name']),\n",
    "                header=False, index=False)\n",
    "\n",
    "    x_true.to_csv(os.path.join(output_path, 'x_true_%s_xyz_nearest.csv' % configs['model']['name']),\n",
    "                header=False, index=False)\n",
    "\n",
    "print('[Output] Results saved at %', output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dados originais\n",
    "N_tracks = 50\n",
    "path_html = ''\n",
    "name = configs['model']['name']\n",
    "\n",
    "if cylindrical:\n",
    "    path_html=os.path.join(output_path, 'track-original-%s_cylin_nearest.html' % name)\n",
    "else:\n",
    "    path_html=os.path.join(output_path, 'track-original-%s_xyz_nearest.html' % name)\n",
    "    \n",
    "fig = track_plot_xyz([data_true], n_hits = 10, cylindrical = cylindrical, n_tracks = 50, \n",
    "               path=path_html, title='Track Original #10 Hit - Model %s' % name.upper())\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted data\n",
    "N_tracks = 8\n",
    "path_html = ''\n",
    "name = configs['model']['name']\n",
    "\n",
    "if cylindrical:\n",
    "    path_html=os.path.join(output_path, 'track-predicted-%s_cylin_normal-pred.html' % name)\n",
    "else:\n",
    "    path_html=os.path.join(output_path, 'track-predicted-%s_xyz_normal-pred.html' % name)\n",
    "    \n",
    "fig = track_plot_xyz([data_true, data_pred], n_hits = 10, cylindrical = cylindrical, n_tracks = N_tracks, \n",
    "               path=path_html, title='Track Prediction #10 Hit - Model %s (Nearest hits)' % name.upper(), auto_open=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted data\n",
    "N_tracks = 8\n",
    "path_html = ''\n",
    "name = configs['model']['name']\n",
    "\n",
    "if cylindrical:\n",
    "    path_html=os.path.join(output_path, 'track-predicted-%s_cylin_nearest.html' % name)\n",
    "else:\n",
    "    path_html=os.path.join(output_path, 'track-predicted-%s_xyz_nearest.html' % name)\n",
    "\n",
    "\n",
    "fig = track_plot_xyz([data_true, data_pred_nearest], n_hits = 10, cylindrical = cylindrical, n_tracks = N_tracks, \n",
    "               path=path_html, title='Track Prediction #10 Hit - Model %s (Nearest hits)' % name.upper(), auto_open=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted data\n",
    "N_tracks = 8\n",
    "path_html = ''\n",
    "name = configs['model']['name']\n",
    "\n",
    "if cylindrical:\n",
    "    path_html=os.path.join(output_path, 'track-predicted-%s_cylin_nearest.html' % name)\n",
    "else:\n",
    "    path_html=os.path.join(output_path, 'track-predicted-%s_xyz_nearest.html' % name)\n",
    "\n",
    "\n",
    "fig = track_plot_xyz([data_true, data_pred, data_pred_nearest], n_hits = 10, cylindrical = cylindrical, n_tracks = N_tracks, \n",
    "               path=path_html, title='Track Prediction #10 Hit - Model %s (Nearest hits)' % name.upper(), auto_open=True)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
